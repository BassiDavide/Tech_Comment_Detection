{
  "max_length": 320,
  "batch_size": 8,
  "gradient_accumulation_steps": 1,
  "learning_rate": 2.6049078843086554e-05,
  "num_epochs": 28,
  "threshold": 0.35,
  "dropout_prob": 0.2,
  "weight_decay": 0.03
}